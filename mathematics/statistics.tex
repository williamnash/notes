\chapter{Statistics}
Data are collections of information made through observations. Statistics is the device through which data is cranked through to make interpretations of what the data actually says.

\subsection{Mean}
Given a set of data, one important feature one might wish to know is what this data looks like on average. This average is typically synonymous with the arithmetic mean defined as 

\begin{align}
\E[x] \equiv \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i
\end{align}
Where $n$ is the total amount of points in the data set.
\subsection{Population vs Sample}
The \textbf{population} is the full body of data points that exist for a given study. For instance if you wanted to measure the variation in height of humans, the population variance would necessarily require that you measure all humans for which this study pertains to. Obviously in real life, this is a tough thing to do, so most often a \textbf{sample} is used. A sample is a subset of the full population.

TODO Derive how you get different variances

\subsection{Variance}
TODO
\subsection{Covariance}
The covariance between two variables tells us how closely they track with each other. The \textbf{population covariance} is defined as 
\begin{align}
\Cov[x,y] = \frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})
\end{align}
Whereas the \textbf{sample covariance} is defined as 
\begin{align}
	\Cov[x,y] = \frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})
\end{align}
TODO: Difference between sample and population.

If the covariance is 0, both variables are independent. If the covariance is large and positive, a larger $x$ value suggests a larger $y$ value (and vice versa). If the coviarance is negative, a larger $x$ value suggests a smaller $y$ value (and vice versa).


\subsubsection{Covariance Matrix}\label{covmat}
With many variables (say $n$ of them), it is often helpful to define the sample Covariance matrix which calculates the covariance between each of the variables $x_i$ with $i = 1,\dots, n$

\begin{align}
\textbf{K}_{x_i,x_j} = \begin{pmatrix} 
                             \Cov[x_0,x_0]&\Cov[x_1,x_0]&\Cov[x_2,x_0]&\cdots &\Cov[x_n,x_0]\\
                             \Cov[x_0,x_1]&\Cov[x_1,x_1]&\Cov[x_2,x_1]&\cdots &\Cov[x_n,x_1]\\
                             \Cov[x_0,x_2]&\Cov[x_1,x_2]&\Cov[x_2,x_2]&\cdots &\Cov[x_n,x_2]\\
                             \vdots&\vdots&\vdots&\ddots&\vdots\\
                             \Cov[x_0,x_n]&\Cov[x_1,x_n]&\Cov[x_2,x_n]&\cdots &\Cov[x_n,x_n] \end{pmatrix}
\end{align}


\label{ch:statistics}

\section{Markov Chains}
TODO

\subsection{Gibbs Sampling}
Gibbs sampling is used to approximate a multivariate probability distribution when directly sampling it is difficult. 
TODO

\section{Probability}
The mathematics of probability translate chances into dice rolls. For a given event, a value $P\in[0,1]$ is assigned to the fraction of times it is expected to happen. The theoretical underpinnings of probability have remained unresolved since it's birth, leaving two primary camps of practitioners; Bayesians and Frequentists.
TODO Bayesian vs Frequentist
"In particle physics, with its strong tradition of frequentist coverage, prior pdfs are often chosen to provide intervals (in particular upper limits for Poisson means) with good frequentist coverage [4]. In such cases, our use of Bayesian computational machinery for interval estimation is not so much a change in paradigm as it is a technical device for frequentist inference." - Bob Cousins \url{https://ep-news.web.cern.ch/node/3213}
\subsubsection{Bayesian Probability}
TODO
\subsubsection{Frequentists Probability}
TODO

\subsection{Random Variables}
"A random variable is one whose values have an associated probability distribution".\cite{grus}
\subsubsection{Asimov Dataset}
An Asimov dataset is an artificial dataset such that when it is used to evaluate the estimators for all parameters in the context of maximum likelihood estimation, one obtains the (assumed) true parameter values. This is typically done to derive $\sigma$ of the parameter of interest $\mu$, which is then used in asymptotic formulae. Some notes on notation
\begin{itemize}
	\item $\hat{\mu}$: Best estimate on the parameter of interest $\mu$
	\item $\mu'$: Assumed true mean of the Gaussian used in Asymptotic formula approximations
\end{itemize}

\subsection{Marginalizing A Variable}
TODO

\subsection{Neyman Pearson Lemma}
The main idea is that you want to maximize your detection probability $P_D$ under then constraint that your Type I error (false alarm) probability $P_{FA}$ is equal to a set value $\alpha$. This is done looking for a maximum with a Lagrange multiplier. TODO

\begin{equation}
	\textrm{max}[P_D - \gamma(P_{FA}-\alpha)]
\end{equation}

Given two simple hypotheses $\theta_0$ (the null hypothesis) and $\theta$, (simple hypotheses specify uniquely the probability distributions), we pick 

\subsection{Hoeffding's Inequality}\label{hoeffding}
Given $X_1, X_2, \ldots, X_n$ independent random variables each within $0 \leq X_i \leq 1$, Hoeffding's inequality states that 
\begin{align}
	P(\bar{X} - E[\bar{X}] \geq t) \leq e^{-2nt^2}
\end{align}
with 
\begin{align}
	\bar{X} = \frac{1}{n}\Big(X_1+X_2+\ldots+X_n\Big)
\end{align}
this gives us an easily computable way to find edge's of cumulative distributions.
 
\subsection{Cross Entropy}
TODO
If we are maximizing for probabilities, using cross-entropy as a loss function represents the negative log-likelihood of the observed data\cite{grus}.

\subsection{Characteristic Function}
The characteristic function is the Fourier transform of a distributions PDF $p(x)$. Defined as 
\begin{align}
\phi_x(k) = E[e^{ikx}] = \int_{-\infty}^\infty e^{ikx}p(x)dx
\end{align}
This function has a one to one relationship with the PDF, and is useful in proving things. It turns out that when adding a whole bunch of random numbers $x_1, x_2, ..., x_n$, all pulled from different PDFs $p_1(x_1), p_2(x_2), ..., p_n(x_n)$
\begin{align}
s = x_1 + x_2 + ... + x_n
\end{align}
the characteristic function has the property that
\begin{align}
\phi_s(k) =\phi_1(k)\phi_2(k)...\phi_n(k)
\end{align}


\subsection{Central Limit Theorem}
States that if you have a bunch of random variables $x_1, x_2, ... x_n$, each coming from an arbitrary pdf with mean $\mu_1, \mu_2, ... \mu_n$, and variance $\sigma_1^2, \sigma_2^2, ...\sigma_n^2$, if you add them all up in the limit of large $n$, it should approach a Gaussian distribution with mean $\mu = \sum_i \mu_i$ and variance $\sigma^2 = \sum_i\sigma_i^2$

TODO

Jeffery's prior allows two people to come up with an equal parameter(?) model that looks different, but have the statistics for the two come out the same, when they otherwise wouldn't (Jacobian, etc.)

\subsection{Laplace's Rule of Succession}
Consider the problem of flipping a coin $N$ times and getting $A$ heads. If each flip has a probability $\rho$ to come up heads, the probability for any amount of heads is given by the Binomial distribution
\begin{align}
p(A) = {N\choose A}\rho^A(1-\rho)^{N-A}
\end{align}
If we want to figure out how fair the coin is, i.e. the value of $\rho$, we know that the probability of a given $\rho$ should be proportional to this function as we
\begin{align}
	p(\rho) \propto {N\choose A}\rho^A(1-\rho)^{N-A}
\end{align}
We also know that $\rho$ must be in $[0,1]$. Knowing that the sum of probabilities over $\rho$ must equal 1, the normalization constant must be
\begin{align}
	p(\rho) =\ffrac{ {N\choose A}\rho^A(1-\rho)^{N-A}}{\int_0^1{N\choose A}\rho^A(1-\rho)^{N-A} d\rho}
\end{align}
Since ${N\choose A}$ is constant and common in both numerator and denominator, we have
\begin{align}
	p(\rho) =\ffrac{ \rho^A(1-\rho)^{N-A}}{\int_0^1\rho^A(1-\rho)^{N-A} d\rho}
\end{align}
It so happens the integral is Euler's beta function\footnote{todo}, which evaluates to 
\begin{align}
	\int_0^1\rho^A(1-\rho)^{N-A} d\rho = \frac{A!(N-A)!}{(N+1)!}
\end{align}
This then simplifies our expression with
\begin{align}
	p(\rho) =(N+1){N\choose A}\rho^A(1-\rho)^{N-A}
\end{align}
This then literally tells us what the chances are of a given $\rho$ given any number of flips $N$ and heads $A$. The question answered by Laplace's rule of succession answers: If I flip a coin $N$ times with $A$ heads, what is the probability of getting heads on the next flip $N+1$?

This is the same thing as asking, what is $\rho$, which is the probability of getting a heads on any single flip. Therefore, all we need to do is take the expectation value of $\rho$
\begin{align}
	\langle \rho\rangle &= \int_0^1\rho ~p(\rho) d\rho\\
	&= (N+1){N\choose A}\int_0^1 ~\rho^{A+1}(1-\rho)^{N-A} d\rho
\end{align}
Using the same beta function relation (or alternatively using the chain rule) we get
\begin{align}
	\langle \rho\rangle &= \int_0^1\rho ~p(\rho) d\rho\\
	&= (N+1)\frac{N!}{A!(N-A)!}\frac{(A+1)!(N-A)!}{(N+2)!}
\end{align}
Therefore, the chance that we get a heads on the next flip, is given by
\begin{align}
	\langle\rho\rangle = \frac{A+1}{N+2}
\end{align}



\subsection{Limits}
Typically in physics papers, we look in regions where we expect no events. Often we are looking for limits on crazy models people cook up that would expect some events there. The number of events should follow a Poisson distribution with $\mu\neq 0$ (forgetting about background). An important value is $\mu=3$, which tells us that there is only a $5\%$ chance that we find 0 events, any higher $\mu$ would have even less of a chance of seeing 0 events. This is obtained by integrating the Poisson distribution.

 In particle physics, $\mu=3$ is held as a the benchmark for if your theory is testable or not, if $\mu < 3$ you have a large chance that even if your theory is correct, you would still see nothing. 


%\subsection{Uncertainties?}
%Study email from Jay "CLCT output to Track Finder" to understand why it makes sense to add errors in quadrature, and write it up here.


\subsection{Bayes Theorem}
The conditional probability, written as $P(A|B)$ is understood as the chance that $A$ happens, given that $B$ has already happened. The chance that both $A$ and $B$ happen $P(A \cap B)$ is the same as the chance $B$ happens at all $P(B)$, multiplied by $P(A|B)$.
\begin{align}
P(A\cap B) = P(A|B)P(B)
\end{align}
This can of course be looked at the other way, with
\begin{align}
P(A\cap B) = P(B|A)P(A)
\end{align}
Therefore
\begin{align}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{align}

\subsection{Likelihood Function}

Start with a probability distribution of some vector of observables $\textbf{x}$, and model parameters $\boldsymbol{\mu}$. Turn the probability distribution into a likelihood function by considering the observables as the objects held constant (since we observed them) and allow our model parameters to vary.

\begin{align}
	P(\textbf{x}|\boldsymbol{\mu}) = \mathcal{L}(\boldsymbol{\mu}|\textbf{x})
\end{align}


Now eliminate all the nuisance parameters to make the Likelihood function only dependent on the quantities you care about ($\boldsymbol{\mu}\rightarrow \mu$) (TODO, see profile likelihood talk). Find the maximum Likelihood estimator with

\begin{align}
\frac{\partial \mathcal{L}}{\partial \mu} = 0 \rightarrow \mu_{ML}
\end{align}
This tells us that if the model had the parameter $\mu_{ML}$ the observation we made would be the most likely, out of the space of all possible $\mu$'s it could have been. To find the \textbf{confidence interval}, we use

\begin{align}
-2\ln\mathcal{L} \simeq \chi^2
\end{align}
This is because (TODO: Learn more about this) most things tend towards Gaussians in the limit of large numbers and in the product of many Likelihood functions one gets
\begin{align}
	-2\ln\prod_i\mathcal{L}_i = -2\ln\prod_i e^{(x_i-\mu)^2/2\sigma^2} = \sum_i\frac{(x_i-\mu)^2}{\sigma^2} = \chi^2
\end{align}
(TODO, $i$'s are correct?). Now using Wilk's Theorem (TODO), one can look at the change in $\chi^2$ as you change the value of $\mu$. Once you increase $\mu$ in one direction from $\mu_{ML}$, the $\chi^2$ will grow. $\chi^2$ will of course also grow if you decrease it from $\mu_{ML}$. You can quantify how much you can change $\mu$ to some limit of $\chi^2$ given by (SOME THEORY). Depending on what kind of confidence interval you are looking for, or the amount of parameters you still have, this change in $\chi^2$ will be different. 

This defines your confidence interval, which tells you, given whatever data you have, what the values of $\mu$,  [$\mu_1, \mu_2$] would be such that what you saw was on the tail end of its distribution (e.g. outside of 95\% of the entire distribution)
\section{Classification}
For classification tasks, the breakdown of how the test performs can be defined using the following quantities
\begin{center}
 \begin{tabular}{||c c||} 
 \hline
Name (Abbr.) & Description \\ [0.5ex] 
 \hline\hline
Positive (P) & Number of real positive cases in the data  \\ 
 \hline
 Negative (N)& Number of real negative cases in the data  \\
 \hline
 True Positive (TP) & Number of positive cases correctly identified  \\
 \hline
True Negative (TN) & Number of negative cases correctly identified \\
 \hline
False Positive (FP) & Number of positive cases incorrectly identified   \\ 
 \hline
 False Negative (FN) &   Number of negative cases incorrectly identified\\
 \hline
\end{tabular}\label{forcing}
\end{center}

\subsection{Accuracy}
A tests accuracy tells you how often you get the right answer, i.e.
\begin{align}
	\textrm{Accuracy} = \frac{TP+TN}{P+N}
\end{align} Typically accuracy is not a particularly important measure if either the negative or positive rate is very large to begin with.

\subsection{Confusion Matrix} 
To determine how strong a classifier is, a confusion matrix can be used to store all of the results.\\
\begin{tabular}{cc|c|c|}
\cline{3-4}
 && \multicolumn{2}{ c| }{Actual Value} \\ \cline{3-4}
 && True & False  \\ \cline{1-4}
\multicolumn{1}{ |c  }{\multirow{2}{*}{Predicted}} &
\multicolumn{1}{ |c| }{True} & TP & FP       \\ \cline{2-4}
\multicolumn{1}{ |c  }{}                        &
\multicolumn{1}{ |c| }{False} & FN & TN     \\ \cline{1-4}
\end{tabular}
\\

From these values we can measure the \textbf{precision} and \textbf{recall} which are both standard ways of measuring a classifiers performance. Precision optimizes for TODO while recall optimizes for TODO

\section{Hypothesis Testing}

\subsection{Confidence Intervals}
TODO
Also add calculation of confidence intervals of fit parameters.

\subsection{P-Value}
The $p$-value is defined as the probability under the null hypothesis of obtaining a result equal to or more extreme than what was actually observed. The smaller the $p$-value, the more it tells investigators that the null hypothesis may not adequately explain the observation.


\subsection{A/B Testing}
See book \cite{grus}


\subsection{F-Test}
TODO

\section{Bootstrapping}
TODO


