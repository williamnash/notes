\chapter{Statistics}
Data are collections of information made through observations. Statistics is the device through which data is cranked through to make interpretations of what the data actually says.

\section{Fundamentals}
\subsection{TODO}
\begin{itemize}
	\item Uniformize notation, use $w$ as a vector, etc
	\item Sort sections
	\item Fill out TODOs
\end{itemize}

\subsection{Mean}
Given a set of data, one important feature one might wish to know is what this data looks like on average. This average is typically synonymous with the arithmetic mean defined as 

\begin{align}
\E[x] \equiv \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i
\end{align}
Where $n$ is the total amount of points in the data set.
\subsection{Population vs Sample}
The \textbf{population} is the full body of data points that exist for a given study. For instance if you wanted to measure the variation in height of humans, 
the population variance would necessarily require that you measure all humans for which this study pertains to. Obviously in real life, this is a tough thing to do, 
so most often a \textbf{sample} is used. A sample is a subset of the full population. For the full population $N$, the mean is a \emph{parameter}, often defined as $\mu$.
For a sample $n$, the mean is a \emph{statistic}, often defined as $\bar{x}$.

\begin{align}
\mu = \E[x] \qquad \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i
\end{align}





TODO Derive how you get different variances

\subsection{Variance}
TODO
\subsection{Covariance}
The covariance between two variables tells us how closely they track with each other. The \textbf{population covariance} is defined as 
\begin{align}
\Cov[x,y] = \frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})
\end{align}
Whereas the \textbf{sample covariance} is defined as 
\begin{align}
	\Cov[x,y] = \frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})
\end{align}
TODO: Difference between sample and population.

If the covariance is 0, both variables are independent. If the covariance is large and positive, a larger $x$ value suggests a larger $y$ value (and vice versa). If the coviarance is negative, a larger $x$ value suggests a smaller $y$ value (and vice versa).


\subsubsection{Covariance Matrix}\label{covmat}
With many variables (say $n$ of them), it is often helpful to define the sample Covariance matrix which calculates the covariance between each of the variables $x_i$ with $i = 1,\dots, n$

\begin{align}
\textbf{K}_{x_i,x_j} = \begin{pmatrix} 
                             \Cov[x_0,x_0]&\Cov[x_1,x_0]&\Cov[x_2,x_0]&\cdots &\Cov[x_n,x_0]\\
                             \Cov[x_0,x_1]&\Cov[x_1,x_1]&\Cov[x_2,x_1]&\cdots &\Cov[x_n,x_1]\\
                             \Cov[x_0,x_2]&\Cov[x_1,x_2]&\Cov[x_2,x_2]&\cdots &\Cov[x_n,x_2]\\
                             \vdots&\vdots&\vdots&\ddots&\vdots\\
                             \Cov[x_0,x_n]&\Cov[x_1,x_n]&\Cov[x_2,x_n]&\cdots &\Cov[x_n,x_n] \end{pmatrix}
\end{align}


\section{Error Propagation}

Given a function $f(x_1, x_2, \dots, x_n)$, the error propagation formula tells us how the error in the output $\sigma_f$ depends on the errors in the inputs $\sigma_{x_i}$. This
is derived using the Taylor expansion of $f$ around the mean values of the inputs $\bar{x_i}$. The first order expansion is given by

\begin{align}\label{taylor_error}
f(x_1, x_2, \dots, x_n) \approx f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + \sum_{i=1}^n \frac{\partial f}{\partial x_i}\Bigg|_{\bar{x_i}}(x_i-\bar{x_i})
\end{align}

Recall the definition of the variance of a random variable $f$ is given by

\begin{align}
\sigma_f^2 = \E[(f-\E[f])^2] = \E[f^2] - \E[f]^2
\end{align}


First, we note that $\E[f] = f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n})$ since the expectation value of a function is the function evaluated at the expectation values of the inputs.
The first term in the variance can be written by using Equation \ref{taylor_error} and the definition of the variance as

\begin{align}
\E[f^2] &= \E\Bigg[\Big(f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + \sum_{i=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}(x_i-\bar{x_i})\Big)^2\Bigg]\\
&= \E\Bigg[f^2(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + 2f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n})\sum_{i=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}(x_i-\bar{x_i}) + \Big(\sum_{i=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}(x_i-\bar{x_i})\Big)^2\Bigg]\\
&= f^2(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + 2f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n})\sum_{i=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\cancel{\E[x_i-\bar{x_i}]}\\
&+ \sum_{i=1}^n \sum_{j=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\frac{\partial f}{\partial x_j}\Big|_{\bar{x_j}}\E[(x_i-\bar{x_i})(x_j-\bar{x_j})]\\
&= f^2(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + \sum_{i=1}^n \sum_{j=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\frac{\partial f}{\partial x_j}\Big|_{\bar{x_j}}\E[(x_i-\bar{x_i})(x_j-\bar{x_j})]
\end{align}

Therefore the variance of $f$ is given by

\begin{align}
\sigma_f^2 &= \sum_{i=1}^n \sum_{j=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\frac{\partial f}{\partial x_j}\Big|_{\bar{x_j}}\E[(x_i-\bar{x_i})(x_j-\bar{x_j})]\\
		&= \sum_{i=1}^n \sum_{j=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\frac{\partial f}{\partial x_j}\Big|_{\bar{x_j}}\Cov[x_i,x_j]
\end{align}

\label{ch:statistics}

\section{Distributions}
\subsection{Binomial Distribution}
The Binomial distribution is given by
\begin{align}
P(k;n,p) = \begin{pmatrix}
n\\
k
\end{pmatrix}p^k(1-p)^{n-k}
\end{align}
Where 
\begin{align}
\begin{pmatrix}
n\\
k
\end{pmatrix} = \frac{n!}{k!(n-k)!}
\end{align}
Which tells us, if we try something $n$ times, each of which has a chance $p$ of succeeding, that we get $k$ successes. 

\subsection{Poisson Distribution}
The Poisson distribution is used to describe the distribution of discrete events over a given time or space interval. Consider a radioactive decay process which has $\lambda$ decays per minute. 

For a process which obey's Binomial statistics, we have a set amount of times the event could happen $n$ and a proportion of successes $k$. For a Poisson process the time or space interval could in principle be broken up infinitely, making it such that we don't just count over a set number of dice rolls, effectively infinitely many of them. Thus we take the limit $n\rightarrow \infty$ of the Binomial distribution with
\begin{align}
p = \frac{\lambda}{n}
\end{align}
Which tells us that the probability of the event happening at any given time is $0$, but as we will see, will give us a property such that a finite amount of events are seen when we are done counting. Mathematically we find

\begin{align}
P(k; \mu) &= \lim_{n\to \infty} \frac{n(n-1)(n-2)...(n-k+1)}{k!}\left(\frac{\lambda}{n}\right)^k\left(1-\frac{\lambda}{n}\right)^{n-k}\\
&= \lim_{n\to \infty}\frac{n^k}{k!}\frac{\lambda^k}{n^k}\left(1-\frac{\lambda}{n}\right)^{n-k}
\end{align}
Using the definition of the exponential function we arrive at the Poisson distribution with
\begin{align}
P(k; \lambda) = \frac{\lambda^ke^{-\lambda}}{k!}
\end{align}

Thus the Poisson distribution is a limiting case of the Binomial distribution when the number of dice rolls is infinite, but the amount of events we observe remain finite.

In counting experiments, it is often useful to know that if 0 events are observed coming from a Poisson distribution, the one-sided 95\% upper limit on the number of events is $\approx 3$, which is consistent with
\begin{align}
	\frac{3^0e^{-3}}{0!} \approx 0.05
\end{align}
Since the confidence interval tells us a model with true parameter $\lambda=3$ has a 5\% chance to see 0 events. In particle physics, $\lambda=3$ is held as a the benchmark for if your theory is testable or not, if $\lambda < 3$ you have a large chance that even if your theory is correct, you would still see nothing. 



\subsection{Gaussian Distribution}
Another distribution shows up in the case where $\lambda$ becomes so large that Stirling's approximation (Section \ref{stirling}) becomes valid. If we take a Poisson distribution with  $k = \lambda(1+\delta) \equiv x$ with $\lambda \gg 1, \delta \ll 1$ we can write
\begin{align}
	P(x; \lambda) &= \frac{\lambda^xe^{-\lambda}}{x!}\\
	&= \frac{\lambda^{\lambda(1+\delta)}e^{-\lambda}}{\lambda(1+\delta)!}
\end{align}
Using Stirling's approximation, one has
\begin{align}
		P(x; \lambda) &\approx \frac{\lambda^{\lambda(1+\delta)}e^{-\lambda}}{\sqrt{2\pi}e^{-\lambda(1+\delta)}(\lambda(1+\delta))^{\lambda(1+\delta)+1/2}}\\
		&\approx \frac{e^{-\lambda\delta}}{\sqrt{2\pi\lambda}} (1+\delta)^{-\lambda(1+\delta)-1/2}
\end{align}
Expanding the last term around $\delta=0$, we have
\begin{align}
	(1+\delta)^{-\lambda(1+\delta)-1/2} &\approx 1 + \delta*(\lambda+1/2) + \dots
\end{align} 
TODO

In summary:
\begin{itemize}
	\item Poisson is limit of binomial for $p$ small and $n\gg \mu$
	\item Gaussian is limit of Poisson for large $\mu$
	\item Gaussian is limit of binomial for large $\mu=np$
\end{itemize}



\section{Markov Chains}
TODO

\subsection{Gibbs Sampling}
Gibbs sampling is used to approximate a multivariate probability distribution when directly sampling it is difficult. 
TODO

\section{Probability}
The mathematics of probability translate chances into dice rolls. For a given event, a value $P\in[0,1]$ is assigned to the fraction of times it is expected to happen. The theoretical underpinnings of probability have remained unresolved since it's birth, leaving two primary camps of practitioners; Bayesians and Frequentists.
TODO Bayesian vs Frequentist
"In particle physics, with its strong tradition of frequentist coverage, prior pdfs are often chosen to provide intervals (in particular upper limits for Poisson means) with good frequentist coverage [4]. In such cases, our use of Bayesian computational machinery for interval estimation is not so much a change in paradigm as it is a technical device for frequentist inference." - Bob Cousins \url{https://ep-news.web.cern.ch/node/3213}
\subsubsection{Bayesian Probability}
TODO
\subsubsection{Frequentists Probability}
TODO



\subsection{Random Variables}
A random variable is one whose values have an associated probability distribution \cite{grus}. Random variables can come in two different flavors: \B{discrete} and \B{continuous}. 

Discrete random variables can only take a countable number of distinct values e.g. ($1,~2, ~5,\ldots$) or (red, blue, green). These values get pulled from something called a \B{probability mass function}.

Conversely, continuous random variables represent a continuous spectrum of values e.g. all the real numbers. These values get pulled from something called a \B{probability density function}.

\subsection{Marginalizing A Variable}
TODO

\subsection{Neyman Pearson Lemma}
The main idea is that you want to maximize your detection probability $P_D$ under then constraint that your Type I error (false alarm) probability $P_{FA}$ is equal to a set value $\alpha$. This is done looking for a maximum with a Lagrange multiplier. TODO

\begin{equation}
	\textrm{max}[P_D - \gamma(P_{FA}-\alpha)]
\end{equation}

Given two simple hypotheses $\theta_0$ (the null hypothesis) and $\theta$, (simple hypotheses specify uniquely the probability distributions), we pick 

\subsection{Hoeffding's Inequality}\label{hoeffding}
Given $X_1, X_2, \ldots, X_n$ independent random variables each within $0 \leq X_i \leq 1$, Hoeffding's inequality states that 
\begin{align}
	P(\bar{X} - E[\bar{X}] \geq t) \leq e^{-2nt^2}
\end{align}
with 
\begin{align}
	\bar{X} = \frac{1}{n}\Big(X_1+X_2+\ldots+X_n\Big)
\end{align}
this gives us an easily computable way to find edge's of cumulative distributions.
 
\subsection{Cross Entropy}
TODO
If we are maximizing for probabilities, using cross-entropy as a loss function represents the negative log-likelihood of the observed data\cite{grus}.

\subsection{Characteristic Function}
The characteristic function is the Fourier transform of a distributions PDF $p(x)$. Defined as 
\begin{align}
\phi_x(k) = E[e^{ikx}] = \int_{-\infty}^\infty e^{ikx}p(x)dx
\end{align}
This function has a one to one relationship with the PDF, and is useful in proving things. It turns out that when adding a whole bunch of random numbers $x_1, x_2, ..., x_n$, all pulled from different PDFs $p_1(x_1), p_2(x_2), ..., p_n(x_n)$
\begin{align}
s = x_1 + x_2 + ... + x_n
\end{align}
the characteristic function has the property that
\begin{align}
\phi_s(k) =\phi_1(k)\phi_2(k)...\phi_n(k)
\end{align}


\subsection{Central Limit Theorem}
States that if you have a bunch of random variables $x_1, x_2, ... x_n$, each coming from an arbitrary pdf with mean $\mu_1, \mu_2, ... \mu_n$, and variance $\sigma_1^2, \sigma_2^2, ...\sigma_n^2$, if you add them all up in the limit of large $n$, it should approach a Gaussian distribution with mean $\mu = \sum_i \mu_i$ and variance $\sigma^2 = \sum_i\sigma_i^2$

TODO

Jeffery's prior allows two people to come up with an equal parameter(?) model that looks different, but have the statistics for the two come out the same, when they otherwise wouldn't (Jacobian, etc.)

\subsection{Laplace's Rule of Succession}
Consider the problem of flipping a coin $N$ times and getting $A$ heads. If each flip has a probability $\rho$ to come up heads, the probability for any amount of heads is given by the Binomial distribution
\begin{align}
p(A) = {N\choose A}\rho^A(1-\rho)^{N-A}
\end{align}
If we want to figure out how fair the coin is, i.e. the value of $\rho$, we know that the probability of a given $\rho$ should be proportional to this function as we
\begin{align}
	p(\rho) \propto {N\choose A}\rho^A(1-\rho)^{N-A}
\end{align}
We also know that $\rho$ must be in $[0,1]$. Knowing that the sum of probabilities over $\rho$ must equal 1, the normalization constant must be
\begin{align}
	p(\rho) =\ffrac{ {N\choose A}\rho^A(1-\rho)^{N-A}}{\int_0^1{N\choose A}\rho^A(1-\rho)^{N-A} d\rho}
\end{align}
Since ${N\choose A}$ is constant and common in both numerator and denominator, we have
\begin{align}
	p(\rho) =\ffrac{ \rho^A(1-\rho)^{N-A}}{\int_0^1\rho^A(1-\rho)^{N-A} d\rho}
\end{align}
It so happens the integral is Euler's beta function, which evaluates to 
\begin{align}
	\int_0^1\rho^A(1-\rho)^{N-A} d\rho = \frac{A!(N-A)!}{(N+1)!}
\end{align}
This then simplifies our expression with
\begin{align}
	p(\rho) =(N+1){N\choose A}\rho^A(1-\rho)^{N-A}
\end{align}
This then literally tells us what the chances are of a given $\rho$ given any number of flips $N$ and heads $A$. The question answered by Laplace's rule of succession answers: If I flip a coin $N$ times with $A$ heads, what is the probability of getting heads on the next flip $N+1$?

This is the same thing as asking, what is $\rho$, which is the probability of getting a heads on any single flip. Therefore, all we need to do is take the expectation value of $\rho$
\begin{align}
	\langle \rho\rangle &= \int_0^1\rho ~p(\rho) d\rho\\
	&= (N+1){N\choose A}\int_0^1 ~\rho^{A+1}(1-\rho)^{N-A} d\rho
\end{align}
Using the same beta function relation (or alternatively using the chain rule) we get
\begin{align}
	\langle \rho\rangle &= \int_0^1\rho ~p(\rho) d\rho\\
	&= (N+1)\frac{N!}{A!(N-A)!}\frac{(A+1)!(N-A)!}{(N+2)!}
\end{align}
Therefore, the chance that we get a heads on the next flip, is given by
\begin{align}
	\langle\rho\rangle = \frac{A+1}{N+2}
\end{align}

%\subsection{Uncertainties?}
%Study email from Jay "CLCT output to Track Finder" to understand why it makes sense to add errors in quadrature, and write it up here.


\subsection{Bayes Theorem}
The conditional probability, written as $P(A|B)$ is understood as the chance that $A$ happens, given that $B$ has already happened. The chance that both $A$ and $B$ happen $P(A \cap B)$ is the same as the chance $B$ happens at all $P(B)$, multiplied by $P(A|B)$.
\begin{align}
P(A\cap B) = P(A|B)P(B)
\end{align}
This can of course be looked at the other way, with
\begin{align}
P(A\cap B) = P(B|A)P(A)
\end{align}
Therefore
\begin{align}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{align}

\section{Likelihood Function}

The likelihood function is an interpretation of the probability distribution which takes the observed values $\textbf{x}$ as \textit{fixed} and allows the parameters $\boldsymbol{\mu}$ to vary.

\begin{align}
	P(\textbf{x}|\boldsymbol{\mu}) = \mathcal{L}(\boldsymbol{\mu}|\textbf{x})
\end{align}

\subsection{Maximum Likelihood Estimation}
The most foolproof means of solving an optimization problem is writing down the corresponding likelihood function, then solving for the parameters $\boldsymbol{\mu}$. If doing so analytically, this corresponds to solving the series of equations given by
\begin{align}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}} = 0
\end{align}
In general this usually isn't possible, so one resorts to other means such as gradient descent. When doing this optimization on a computer, typically one optimizes for
\begin{align}
	\ln\mathcal{L}
\end{align}
instead of $\mathcal{L}$. This is kosher since $\ln$ is a strictly increasing function, so maximizing it means maximizing its arguments \cite{burkov}. There are also some nice properties since 
\begin{align}
-2\ln\mathcal{L} \simeq \chi^2
\end{align}

% TODO Maximum Likelihood Estimation

This tells us that if the model had the parameter $\mu_{ML}$ the observation we made would be the most likely, out of the space of all possible $\mu$'s it could have been. To find the \textbf{confidence interval}, we use

\begin{align}
-2\ln\mathcal{L} \simeq \chi^2
\end{align}
This is because (TODO: Learn more about this) most things tend towards Gaussians in the limit of large numbers and in the product of many Likelihood functions one gets
\begin{align}
	-2\ln\prod_i\mathcal{L}_i = -2\ln\prod_i e^{(x_i-\mu)^2/2\sigma^2} = \sum_i\frac{(x_i-\mu)^2}{\sigma^2} = \chi^2
\end{align}
(TODO, $i$'s are correct?). Now using Wilk's Theorem (TODO), one can look at the change in $\chi^2$ as you change the value of $\mu$. Once you increase $\mu$ in one direction from $\mu_{ML}$, the $\chi^2$ will grow. $\chi^2$ will of course also grow if you decrease it from $\mu_{ML}$. You can quantify how much you can change $\mu$ to some limit of $\chi^2$ given by (SOME THEORY). Depending on what kind of confidence interval you are looking for, or the amount of parameters you still have, this change in $\chi^2$ will be different. 

This defines your confidence interval, which tells you, given whatever data you have, what the values of $\mu$,  [$\mu_1, \mu_2$] would be such that what you saw was on the tail end of its distribution (e.g. outside of 95\% of the entire distribution)

\subsection{Asimov Dataset}
An Asimov dataset is an artificial dataset such that when it is used to evaluate the estimators for all parameters in the context of maximum likelihood estimation, one obtains the (assumed) true parameter values. This is typically done to derive $\sigma$ of the parameter of interest $\mu$, which is then used in asymptotic formulae. Some notes on notation
\begin{itemize}
	\item $\hat{\mu}$: Best estimate on the parameter of interest $\mu$
	\item $\mu'$: Assumed true mean of the Gaussian used in Asymptotic formula approximations
\end{itemize}
This method is used to derive the approximate Gaussian width when calculating Asymptotic formulas for limits in particle physics \cite{cowan}. It was named after author, Isaac Asimov, whose 1955 short story, Franchise, envisaged the 2008 US Presidential Election decided by one voter representative of the entire electorate.



\section{Hypothesis Testing}

\subsection{Confidence Intervals}
TODO
Also add calculation of confidence intervals of fit parameters.
\url{https://online.stat.psu.edu/stat415/lesson/7/7.5}
Add confidence interval of a Gaussian

\subsection{P-Value}
The $p$-value is defined as the probability under the null hypothesis of obtaining a result equal to or more extreme than what was actually observed. The smaller the $p$-value, the more it tells investigators that the null hypothesis may not adequately explain the observation.


\subsection{A/B Testing}
See book \cite{grus}


\subsection{F-Test}
TODO

\section{Kalman Filter}
The Kalman filter\cite{fruwirth} uses a series of measurements taken in a time series to make an estimate of the underlying parameters. It works as an iterative $\chi^2$ method, and is useful when there is noise introduced between measurements.

Let us assume the state vector of interest is $\textbf{x}$. If we assume the evolution of the state vector is linear, we have
\begin{align}
    \textbf{x}_k = \textbf{F}_{k-1}\textbf{x}_{k-1} + w_{k-1}
\end{align}
Where $k$ is the current state, $w$ is random noise that is introduced, and $\textbf{F}$ is how the state vector propagates itself (here it is assumed to be linear). Measurements $\textbf{m}$ in each detector $k$ can be written as
\begin{align}
    \textbf{m}_k = \textbf{H}_k\textbf{x}_k + \epsilon_k
\end{align}
We assume both $w_k$ and $\epsilon_k$ are independent and have a mean of zero. The Kalman algorithm performs three different analyses:
\begin{enumerate}
    \item \textbf{Filtering}: estimation of present state vector based off of all previous measurements.
    \item \textbf{Prediction}: estimation of future state vector.
    \item \textbf{Smoothing}: estimation of the state vector at some time in the past using all information known in the present.
\end{enumerate}
The Kalman filter minimizes the mean squared error and is in that sense the optimum solution to these problems. We can denote our estimate $\hat{\textbf{x}}_k^i$ for each of these three cases:
\begin{itemize}
    \item $i < k$: prediction, since we are using fewer points than the state we are looking at $k$
    \item $i=k$: filtering, since we are using all previous states and the measurement at $k$ itself
    \item $i > k$: smoothing, since we already had a measurement at this point, but used additional later values to get a better estimate
\end{itemize}
One can then define the covariance matrix as the difference between the ...



\section{Bootstrapping}
TODO


