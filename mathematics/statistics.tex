\chapter{Statistics}
Data are collections of information made through observations. Statistics is the device through which data is cranked through to make interpretations of what the data actually says.

\section{Fundamentals}

\subsection{Mean}
Given a set of data, one important feature one might wish to know is what this data looks like on average. This average is typically synonymous with the arithmetic mean defined as 

\begin{align}
\E[x] \equiv \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i
\end{align}
Where $n$ is the total amount of points in the data set.
\subsection{Population vs Sample}
The \textbf{population} is the full body of data points that exist for a given study. For instance if you wanted to measure the variation in height of humans, 
the population variance would necessarily require that you measure all humans for which this study pertains to. Obviously in real life, this is a tough thing to do, 
so most often a \textbf{sample} is used. A sample is a subset of the full population. For the full population $N$, the mean is a \emph{parameter}, often defined as $\mu$.
For a sample $n$, the mean is a \emph{statistic}, often defined as $\bar{x}$.

\begin{align}
\mu = \E[x] \qquad \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i
\end{align}

\subsection{Variance}
The variance measures the spread or variability of data around the mean.

The \textbf{population variance} $\sigma^2$ is defined as:
\begin{align}
\sigma^2 = \frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2
\end{align}

The \textbf{sample variance} $s^2$ is defined as:
\begin{align}
s^2 = \frac{1}{n-1}\sum_{i=1}^n(x_i - \bar{x})^2
\end{align}

The sample variance uses $n-1$ in the denominator (Bessel's correction) to provide an unbiased estimator of the population variance.

\subsection{Covariance}
The covariance between two variables tells us how closely they track with each other. The \textbf{population covariance} is defined as 
\begin{align}
\Cov[x,y] = \frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})
\end{align}
Whereas the \textbf{sample covariance} is defined as 
\begin{align}
	\Cov[x,y] = \frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})
\end{align}

Population covariance considers all members, whereas sample covariance is adjusted by $n-1$ for unbiased estimation.

If covariance is:
\begin{itemize}
    \item Positive: variables tend to increase or decrease together.
    \item Negative: one variable tends to increase when the other decreases.
    \item Zero: variables are independent (no linear relationship).
\end{itemize}

\subsubsection{Covariance Matrix}\label{covmat}
With many variables (say $n$ of them), it is often helpful to define the sample Covariance matrix which calculates the covariance between each of the variables $x_i$ with $i = 1,\dots, n$

\begin{align}
\textbf{K}_{x_i,x_j} = \begin{pmatrix} 
                             \Cov[x_0,x_0]&\Cov[x_1,x_0]&\Cov[x_2,x_0]&\cdots &\Cov[x_n,x_0]\\
                             \Cov[x_0,x_1]&\Cov[x_1,x_1]&\Cov[x_2,x_1]&\cdots &\Cov[x_n,x_1]\\
                             \Cov[x_0,x_2]&\Cov[x_1,x_2]&\Cov[x_2,x_2]&\cdots &\Cov[x_n,x_2]\\
                             \vdots&\vdots&\vdots&\ddots&\vdots\\
                             \Cov[x_0,x_n]&\Cov[x_1,x_n]&\Cov[x_2,x_n]&\cdots &\Cov[x_n,x_n] \end{pmatrix}
\end{align}


\section{Error Propagation}

Given a function $f(x_1, x_2, \dots, x_n)$, the error propagation formula tells us how the error in the output $\sigma_f$ depends on the errors in the inputs $\sigma_{x_i}$. This
is derived using the Taylor expansion of $f$ around the mean values of the inputs $\bar{x_i}$. The first order expansion is given by

\begin{align}\label{taylor_error}
f(x_1, x_2, \dots, x_n) \approx f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + \sum_{i=1}^n \frac{\partial f}{\partial x_i}\Bigg|_{\bar{x_i}}(x_i-\bar{x_i})
\end{align}

Recall the definition of the variance of a random variable $f$ is given by

\begin{align}
\sigma_f^2 = \E[(f-\E[f])^2] = \E[f^2] - \E[f]^2
\end{align}


First, we note that $\E[f] = f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n})$ since the expectation value of a function is the function evaluated at the expectation values of the inputs.
The first term in the variance can be written by using Equation \ref{taylor_error} and the definition of the variance as

\begin{align}
\E[f^2] &= \E\Bigg[\Big(f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + \sum_{i=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}(x_i-\bar{x_i})\Big)^2\Bigg]\\
&= \E\Bigg[f^2(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + 2f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n})\sum_{i=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}(x_i-\bar{x_i})\\
&+ \Big(\sum_{i=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}(x_i-\bar{x_i})\Big)^2\Bigg]\\
&= f^2(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + 2f(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n})\sum_{i=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\cancel{\E[x_i-\bar{x_i}]}\\
&+ \sum_{i=1}^n \sum_{j=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\frac{\partial f}{\partial x_j}\Big|_{\bar{x_j}}\E[(x_i-\bar{x_i})(x_j-\bar{x_j})]\\
&= f^2(\bar{x_1}, \bar{x_2}, \dots, \bar{x_n}) + \sum_{i=1}^n \sum_{j=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\frac{\partial f}{\partial x_j}\Big|_{\bar{x_j}}\E[(x_i-\bar{x_i})(x_j-\bar{x_j})]
\end{align}

Therefore the variance of $f$ is given by

\begin{align}
\sigma_f^2 &= \sum_{i=1}^n \sum_{j=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\frac{\partial f}{\partial x_j}\Big|_{\bar{x_j}}\E[(x_i-\bar{x_i})(x_j-\bar{x_j})]\\
		&= \sum_{i=1}^n \sum_{j=1}^n \frac{\partial f}{\partial x_i}\Big|_{\bar{x_i}}\frac{\partial f}{\partial x_j}\Big|_{\bar{x_j}}\Cov[x_i,x_j]
\end{align}

\label{ch:statistics}

\section{Distributions}
\subsection{Binomial Distribution}
The Binomial distribution is given by
\begin{align}
P(k;n,p) = \begin{pmatrix}
n\\
k
\end{pmatrix}p^k(1-p)^{n-k}
\end{align}
Where 
\begin{align}
\begin{pmatrix}
n\\
k
\end{pmatrix} = \frac{n!}{k!(n-k)!}
\end{align}
Which tells us, if we try something $n$ times, each of which has a chance $p$ of succeeding, that we get $k$ successes. 

\subsection{Poisson Distribution}
The Poisson distribution is used to describe the distribution of discrete events over a given time or space interval. Consider a radioactive decay process which has $\lambda$ decays per minute. 

For a process which obey's Binomial statistics, we have a set amount of times the event could happen $n$ and a proportion of successes $k$. For a Poisson process the time or space interval could in principle be broken up infinitely, making it such that we don't just count over a set number of dice rolls, effectively infinitely many of them. Thus we take the limit $n\rightarrow \infty$ of the Binomial distribution with
\begin{align}
p = \frac{\lambda}{n}
\end{align}
Which tells us that the probability of the event happening at any given time is $0$, but as we will see, will give us a property such that a finite amount of events are seen when we are done counting. Mathematically we find

\begin{align}
P(k; \mu) &= \lim_{n\to \infty} \frac{n(n-1)(n-2)...(n-k+1)}{k!}\left(\frac{\lambda}{n}\right)^k\left(1-\frac{\lambda}{n}\right)^{n-k}\\
&= \lim_{n\to \infty}\frac{n^k}{k!}\frac{\lambda^k}{n^k}\left(1-\frac{\lambda}{n}\right)^{n-k}
\end{align}
Using the definition of the exponential function we arrive at the Poisson distribution with
\begin{align}
P(k; \lambda) = \frac{\lambda^ke^{-\lambda}}{k!}
\end{align}

Thus the Poisson distribution is a limiting case of the Binomial distribution when the number of dice rolls is infinite, but the amount of events we observe remain finite.

In counting experiments, it is often useful to know that if 0 events are observed coming from a Poisson distribution, the one-sided 95\% upper limit on the number of events is $\approx 3$, which is consistent with
\begin{align}
	\frac{3^0e^{-3}}{0!} \approx 0.05
\end{align}
Since the confidence interval tells us a model with true parameter $\lambda=3$ has a 5\% chance to see 0 events. In particle physics, $\lambda=3$ is held as a the benchmark for if your theory is testable or not, if $\lambda < 3$ you have a large chance that even if your theory is correct, you would still see nothing. 



\subsection{Gaussian Distribution}
Another distribution shows up in the case where $\lambda$ becomes so large that Stirling's approximation (Section \ref{stirling}) becomes valid. If we take a Poisson distribution with  $k = \lambda(1+\delta) \equiv x$ with $\lambda \gg 1, \delta \ll 1$ we can write
\begin{align}
	P(x; \lambda) &= \frac{\lambda^xe^{-\lambda}}{x!}\\
	&= \frac{\lambda^{\lambda(1+\delta)}e^{-\lambda}}{\lambda(1+\delta)!}
\end{align}
Using Stirling's approximation, one has
\begin{align}
		P(x; \lambda) &\approx \frac{\lambda^{\lambda(1+\delta)}e^{-\lambda}}{\sqrt{2\pi}e^{-\lambda(1+\delta)}(\lambda(1+\delta))^{\lambda(1+\delta)+1/2}}\\
		&\approx \frac{e^{\lambda\delta}}{\sqrt{2\pi\lambda}} (1+\delta)^{-\lambda(1+\delta)-1/2}
\end{align}
First consider the logarithm of this term
\begin{align}
	\ln\left[(1+\delta)^{-\lambda(1+\delta)-1/2}\right] &= \left[-\lambda(1+\delta)-\frac{1}{2}\right]\ln(1+\delta) \\
	&\approx \left(- \lambda - \lambda\delta - \frac{1}{2}\right)\ln(1+\delta) \\
\end{align}
Expanding around small $\delta$ we have
\begin{align}
	\ln(1+\delta) &\approx \delta - \frac{\delta^2}{2} + \frac{\delta^3}{3} + \dots
\end{align}
Therefore
\begin{align}
	\ln\left[(1+\delta)^{-\lambda(1+\delta)-1/2}\right] &\approx \left(-\lambda - \lambda\delta - \frac{1}{2}\right)\left(\delta - \frac{\delta^2}{2} + \frac{\delta^3}{3} + \dots\right) \\
	&\approx -\lambda\delta - \lambda\frac{\delta^2}{2} - \frac{\delta}{2} + \dots
\end{align}
The first two terms are the largest, re-exponentiating the terms then give us
\begin{align}
	P(x; \lambda) &\approx \frac{e^{\lambda\delta}}{\sqrt{2\pi\lambda}}  e^{-\lambda\delta - \lambda\frac{\delta^2}{2}}\\
	&= \frac{1}{\sqrt{2\pi\lambda}}e^{\frac{-\lambda\delta^2}{2}}  \\
\end{align}
Recall $\delta = (x-\lambda)/\lambda$, thus
\begin{align}
	P(x;\lambda) &\approx \frac{1}{\sqrt{2\pi\lambda}}\exp\left(-\frac{(x-\lambda)^2}{2\lambda}\right)
\end{align}

This is the Gaussian (or normal) distribution with mean $\mu = \lambda$ and variance $\sigma^2 = \lambda$. Thus, we explicitly see how the Poisson distribution approaches the Gaussian distribution for large mean $\lambda$.




\section{Markov Chains}
Markov chains are a class of stochastic processes satisfying the \textit{Markov property}, which states that the probability distribution of a future state depends only on the current state and not on the sequence of events that preceded it. Formally, a discrete-time Markov chain is a sequence of random variables
\[
X_0, X_1, X_2, \dots
\]
where each \(X_i\) takes values in some state space (e.g.\ a finite or countable set), and
\[
P\big(X_{n+1} = x_{n+1} \,\big|\, X_n = x_n, X_{n-1} = x_{n-1}, \dots, X_0 = x_0\big)
= P\big(X_{n+1} = x_{n+1} \,\big|\, X_n = x_n\big).
\]
This condition is referred to as the \textit{Markov property}.

\subsection{Transition probabilities and stationarity}
A Markov chain is fully specified by its transition probabilities, which in discrete state spaces can be summarized by a transition matrix
\[
P_{ij} = P\big(X_{n+1} = j \,\big|\, X_n = i\big).
\]
Given initial probabilities \( \pi^{(0)} \) for \( X_0 \), the distribution for \(X_n\) evolves according to
\[
\pi^{(n+1)} = \pi^{(n)} P,
\]
where \(\pi^{(n)}\) is a row vector whose \(i\)-th entry is \(P(X_n = i)\). A distribution \(\pi\) is called a \textit{stationary distribution} of the Markov chain if
\[
\pi P = \pi,
\]
i.e.\ applying the transition probabilities to \(\pi\) leaves it invariant. Under certain conditions (such as the chain being \textit{irreducible} and \textit{aperiodic}), the Markov chain will converge to this unique stationary distribution regardless of the initial state.

\subsection{Markov Chain Monte Carlo (MCMC)}
In many applications, we wish to generate samples from a complicated or high-dimensional probability distribution \(\pi(x)\). Direct sampling can be challenging, but if we can construct a Markov chain whose stationary distribution is \(\pi(x)\), then running that chain for many steps will produce samples that (after an initial ``burn-in'' phase) approximate draws from \(\pi(x)\). This broad class of methods is known as Markov Chain Monte Carlo (MCMC). One of the most popular MCMC algorithms is \textit{Gibbs sampling}, described next.

\subsection{Gibbs Sampling}
Gibbs sampling is an MCMC algorithm specifically suited to sampling from \(\pi(x_1, x_2, \ldots, x_n)\) when we can easily sample from each of the corresponding \textit{full conditional} distributions:
\[
\pi\bigl(x_1 \,\big|\, x_2, \dots, x_n\bigr),\quad
\pi\bigl(x_2 \,\big|\, x_1, x_3, \dots, x_n\bigr),\quad \dots,\quad
\pi\bigl(x_n \,\big|\, x_1, \dots, x_{n-1}\bigr).
\]
The key idea of Gibbs sampling is that rather than attempt to sample all variables jointly (which might be difficult), we iteratively sample each variable from its \emph{conditional} distribution, given all the others. 

\paragraph{Algorithm (Gibbs Sampler)}
Suppose we want to sample from a joint distribution \(\pi(x_1, \ldots, x_n)\). Gibbs sampling proceeds as follows:

\begin{enumerate}
    \item \textbf{Initialize} all variables: choose a starting point \(\bigl(x_1^{(0)}, \dots, x_n^{(0)}\bigr)\).
    \item For each iteration \(t = 1, 2, \dots\):
    \begin{enumerate}
        \item Sample \( x_1^{(t)} \) from \(\pi\bigl(x_1 \,\big|\, x_2^{(t-1)}, \dots, x_n^{(t-1)}\bigr)\).
        \item Sample \( x_2^{(t)} \) from \(\pi\bigl(x_2 \,\big|\, x_1^{(t)}, x_3^{(t-1)}, \dots, x_n^{(t-1)}\bigr)\).
        \item Continue similarly for \( x_3^{(t)}, \dots, x_n^{(t)}\), each time conditioning on the most recently updated values of the other variables.
    \end{enumerate}
\end{enumerate}

After a sufficiently large number of iterations, the distribution of \(\bigl(x_1^{(t)}, \dots, x_n^{(t)}\bigr)\) will be close to the \textit{stationary} distribution \(\pi(x_1, \dots, x_n)\). The chain can be \emph{thinned} by taking every \(k\)-th sample and discarding the initial samples (the \emph{burn-in} period), to reduce correlation and ensure approximate independence from the initialization.

\paragraph{Why it works}
Gibbs sampling is a special case of the Metropolis--Hastings algorithm where the \emph{proposal} for each variable is exactly its full conditional. Under mild regularity conditions (irreducibility, aperiodicity), the Markov chain defined by these updates converges to \(\pi(x_1, \dots, x_n)\). This allows us to obtain (correlated) samples from complicated high-dimensional distributions by sequentially sampling from simpler one-dimensional or lower-dimensional conditional distributions.

\paragraph{Applications}
Gibbs sampling is especially powerful when the conditional distributions \(\pi\bigl(x_i \mid x_{-i}\bigr)\) are of known form (e.g.\ Gaussian, Gamma, Beta, etc.), making each step a direct draw from a standard distribution. It is widely used in Bayesian statistics, image processing (e.g.\ for hidden Markov random fields), and any domain where full-conditionals are tractable but the joint distribution is not easily sampled from in a single step.

\section{Probability}
The mathematics of probability translate chances into dice rolls. For a given event, a value $P\in[0,1]$ is assigned to the fraction of times it is expected to happen. The theoretical underpinnings of probability have remained unresolved since its birth, leaving two primary camps of practitioners: Bayesians and Frequentists.

\subsection{Bayesian Probability}
The Bayesian interpretation treats probability as a \emph{degree of belief} in a proposition. Under this framework, probabilities are subjective and can be updated as new evidence is acquired. The core of Bayesian reasoning is Bayes' theorem:
\begin{align}
	P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)}
\end{align}
where:
\begin{itemize}
	\item $P(\theta)$ is the \textbf{prior}: our initial belief about parameter $\theta$ before seeing data
	\item $P(D | \theta)$ is the \textbf{likelihood}: the probability of observing data $D$ given $\theta$
	\item $P(\theta | D)$ is the \textbf{posterior}: our updated belief after seeing the data
	\item $P(D)$ is the \textbf{evidence} or marginal likelihood, serving as a normalization constant
\end{itemize}

The Bayesian approach allows incorporation of prior knowledge and provides a natural framework for updating beliefs. It also gives a full probability distribution over parameters rather than just point estimates.

\textit{``In particle physics, with its strong tradition of frequentist coverage, prior pdfs are often chosen to provide intervals (in particular upper limits for Poisson means) with good frequentist coverage. In such cases, our use of Bayesian computational machinery for interval estimation is not so much a change in paradigm as it is a technical device for frequentist inference.''} - Bob Cousins \cite{cousins}

\subsection{Frequentist Probability}
The Frequentist interpretation defines probability as the \emph{long-run frequency} of an event occurring in repeated trials. Under this view, probability is an objective property of the physical world, not a degree of belief.

Key characteristics of the frequentist approach:
\begin{itemize}
	\item Parameters are fixed (but unknown) constants, not random variables
	\item Probabilities are defined only for repeatable random events
	\item Inference is based on the sampling distribution of estimators
	\item Results are interpreted in terms of what would happen over many repetitions of an experiment
\end{itemize}

Frequentist methods include hypothesis testing (with $p$-values) and confidence intervals. A 95\% confidence interval, for example, means that if we repeated the experiment many times, 95\% of the constructed intervals would contain the true parameter value. Importantly, this is \emph{not} the same as saying there's a 95\% probability the true value lies in any particular interval---that would be a Bayesian statement.



\subsection{Random Variables}
A random variable is one whose values have an associated probability distribution \cite{grus}. Random variables can come in two different flavors: \B{discrete} and \B{continuous}. 

Discrete random variables can only take a countable number of distinct values e.g. ($1,~2, ~5,\ldots$) or (red, blue, green). These values get pulled from something called a \B{probability mass function}.

Conversely, continuous random variables represent a continuous spectrum of values e.g. all the real numbers. These values get pulled from something called a \B{probability density function}.

\subsection{Marginalizing a Variable}
Given a joint probability distribution $P(X, Y)$, we can obtain the marginal distribution of one variable by summing (or integrating) over all possible values of the other. For discrete random variables:
\begin{align}
	P(X) = \sum_y P(X, Y=y)
\end{align}
For continuous random variables:
\begin{align}
	p(x) = \int_{-\infty}^{\infty} p(x, y) \, dy
\end{align}

This process is called \textbf{marginalization} and effectively ``averages out'' the variable we don't care about. The name comes from the fact that if you write out a joint probability table, summing across rows or columns gives you the marginal probabilities in the margins of the table.

Marginalization is essential in Bayesian inference when we have nuisance parameters $\boldsymbol{\nu}$ that we want to eliminate:
\begin{align}
	P(\theta | D) = \int P(\theta, \boldsymbol{\nu} | D) \, d\boldsymbol{\nu}
\end{align}
This integral can often be intractable analytically, which is one motivation for Monte Carlo methods like MCMC.

\subsection{Neyman-Pearson Lemma}

The main idea is that you want to maximize your detection probability $P_D$ under the constraint that your Type I error (false alarm) probability $P_{FA}$ is equal to some preset value $\alpha$. This is typically done by introducing a Lagrange multiplier and solving an optimization problem:

\begin{equation}
	\max \left[ P_D - \gamma (P_{FA} - \alpha) \right]
\end{equation}

Here, $\gamma$ is the Lagrange multiplier enforcing the constraint $P_{FA} = \alpha$.

This setup leads to the Neyman-Pearson Lemma, which provides a powerful result in hypothesis testing. Given two simple hypotheses:
\begin{itemize}
	\item $\theta_0$: the null hypothesis (e.g., no signal)
	\item $\theta_1$: the alternative hypothesis (e.g., signal present)
\end{itemize}
where both hypotheses fully specify their respective probability distributions, the lemma states that the likelihood ratio test is the most powerful test for a given false alarm rate $\alpha$.

Define the likelihood ratio:
\begin{equation}
	\Lambda(x) = \frac{p(x | \theta_1)}{p(x | \theta_0)}
\end{equation}

Then the most powerful test of size $\alpha$ is:
\[
\Lambda(x) \underset{\theta_0}{\overset{\theta_1}{\gtrless}} \eta
\]
where the threshold $\eta$ is chosen such that $P_{FA} = \alpha$.

In words: we compare how likely the observed data $x$ is under the alternative hypothesis vs. the null, and we decide in favor of $\theta_1$ (detection) if this ratio exceeds some threshold. The Neyman-Pearson result guarantees that this test maximizes $P_D$ for a fixed $P_{FA}$.

This forms the basis of many detection systems, where minimizing missed detections while controlling false alarms is critical.

\subsection{Hoeffding's Inequality}\label{hoeffding}
Given $X_1, X_2, \ldots, X_n$ independent random variables each within $0 \leq X_i \leq 1$, Hoeffding's inequality states that 
\begin{align}
	P(\bar{X} - E[\bar{X}] \geq t) \leq e^{-2nt^2}
\end{align}
with 
\begin{align}
	\bar{X} = \frac{1}{n}\Big(X_1+X_2+\ldots+X_n\Big)
\end{align}
this gives us an easily computable way to find edge's of cumulative distributions.
 
\subsection{Cross Entropy}
The cross entropy between two probability distributions $p$ and $q$ over the same set of events measures the average number of bits needed to encode events from $p$ using an optimal code for $q$. It is defined as:
\begin{align}
	H(p, q) = -\sum_x p(x) \log q(x)
\end{align}
For continuous distributions:
\begin{align}
	H(p, q) = -\int p(x) \log q(x) \, dx
\end{align}

Cross entropy is closely related to the \textbf{Kullback-Leibler (KL) divergence}, which measures how different $q$ is from $p$:
\begin{align}
	D_{KL}(p \| q) = H(p, q) - H(p)
\end{align}
where $H(p) = -\sum_x p(x) \log p(x)$ is the entropy of $p$. Since $H(p)$ is constant with respect to $q$, minimizing cross entropy is equivalent to minimizing KL divergence.

In machine learning, cross entropy is commonly used as a loss function for classification. If $y$ is the true label (often one-hot encoded) and $\hat{y}$ is the predicted probability distribution, the cross-entropy loss is:
\begin{align}
	\mathcal{L} = -\sum_i y_i \log \hat{y}_i
\end{align}

If we are maximizing for probabilities, using cross-entropy as a loss function represents the negative log-likelihood of the observed data \cite{grus}. For binary classification with true label $y \in \{0, 1\}$ and predicted probability $\hat{p}$, this reduces to:
\begin{align}
	\mathcal{L} = -y \log \hat{p} - (1 - y) \log(1 - \hat{p})
\end{align}

\subsection{Characteristic Function}
The characteristic function is the Fourier transform of a distributions PDF $p(x)$. Defined as 
\begin{align}
\phi_x(k) = E[e^{ikx}] = \int_{-\infty}^\infty e^{ikx}p(x)dx
\end{align}
This function has a one to one relationship with the PDF, and is useful in proving things. It turns out that when adding a whole bunch of random numbers $x_1, x_2, ..., x_n$, all pulled from different PDFs $p_1(x_1), p_2(x_2), ..., p_n(x_n)$
\begin{align}
s = x_1 + x_2 + ... + x_n
\end{align}
the characteristic function has the property that
\begin{align}
\phi_s(k) =\phi_1(k)\phi_2(k)...\phi_n(k)
\end{align}


\subsection{Central Limit Theorem}

States that if you have a bunch of random variables $x_1, x_2, \dots, x_n$, each coming from an arbitrary PDF with mean $\mu_1, \mu_2, \dots, \mu_n$ and variance $\sigma_1^2, \sigma_2^2, \dots, \sigma_n^2$, then their sum (or average) will approach a Gaussian distribution in the limit of large $n$. This is essentially the content of the Central Limit Theorem.

More precisely, let
\[
S_n = \sum_{i=1}^n x_i
\]
Then, for large $n$, the distribution of $S_n$ approaches
\[
S_n \sim \mathcal{N}\left( \mu = \sum_i \mu_i, \quad \sigma^2 = \sum_i \sigma_i^2 \right)
\]
The key idea is that even if each $x_i$ comes from some weird, lumpy, or heavy-tailed distribution (as long as the variances are finite), their sum smooths out and becomes bell-shaped. This explains why the Gaussian distribution is so ubiquitous — adding enough random stuff tends to make things normal.

If the variables are i.i.d. (independent and identically distributed), then the statement becomes:
\[
S_n \sim \mathcal{N}(n\mu, n\sigma^2)
\quad \text{or} \quad
\bar{x} = \frac{S_n}{n} \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})
\]

\subsection{Jeffreys Prior}

Jeffreys prior shows up when trying to make "uninformative" or "objective" choices in Bayesian inference. The problem is this: suppose two people model the same thing using different parameterizations. One uses $\theta$, the other uses $\phi = f(\theta)$. If they both assign a flat prior (i.e., "I have no idea what the parameter is") in their respective coordinate systems, they could end up making different inferences — just because of a change of variable. That's not good.

Jeffreys solved this by proposing a prior that’s invariant under reparameterization. That is, it adjusts for the "distortion" introduced by transforming coordinates. The Jeffreys prior is defined as:
\[
p_J(\theta) \propto \sqrt{I(\theta)}
\]
where $I(\theta)$ is the Fisher information:
\[
I(\theta) = \mathbb{E} \left[ \left( \frac{\partial}{\partial \theta} \log p(x|\theta) \right)^2 \right]
\]

This quantity transforms nicely under reparameterization (specifically, it scales with the Jacobian squared), so the square root of it ensures that your prior doesn’t depend on the coordinate system you happened to choose.

In short: Jeffreys prior fixes the issue where “uninformed” priors aren’t truly uninformed, unless you account for the geometry of the parameter space.

\subsection{Laplace's Rule of Succession}
Consider the problem of flipping a coin $N$ times and getting $A$ heads. If each flip has a probability $\rho$ to come up heads, the probability for any amount of heads is given by the Binomial distribution
\begin{align}
p(A) = {N\choose A}\rho^A(1-\rho)^{N-A}
\end{align}
If we want to figure out how fair the coin is, i.e. the value of $\rho$, we know that the probability of a given $\rho$ should be proportional to this function as we
\begin{align}
	p(\rho) \propto {N\choose A}\rho^A(1-\rho)^{N-A}
\end{align}
We also know that $\rho$ must be in $[0,1]$. Knowing that the sum of probabilities over $\rho$ must equal 1, the normalization constant must be
\begin{align}
	p(\rho) =\ffrac{ {N\choose A}\rho^A(1-\rho)^{N-A}}{\int_0^1{N\choose A}\rho^A(1-\rho)^{N-A} d\rho}
\end{align}
Since ${N\choose A}$ is constant and common in both numerator and denominator, we have
\begin{align}
	p(\rho) =\ffrac{ \rho^A(1-\rho)^{N-A}}{\int_0^1\rho^A(1-\rho)^{N-A} d\rho}
\end{align}
It so happens the integral is Euler's beta function, which evaluates to 
\begin{align}
	\int_0^1\rho^A(1-\rho)^{N-A} d\rho = \frac{A!(N-A)!}{(N+1)!}
\end{align}
This then simplifies our expression with
\begin{align}
	p(\rho) =(N+1){N\choose A}\rho^A(1-\rho)^{N-A}
\end{align}
This then literally tells us what the chances are of a given $\rho$ given any number of flips $N$ and heads $A$. The question answered by Laplace's rule of succession answers: If I flip a coin $N$ times with $A$ heads, what is the probability of getting heads on the next flip $N+1$?

This is the same thing as asking, what is $\rho$, which is the probability of getting a heads on any single flip. Therefore, all we need to do is take the expectation value of $\rho$
\begin{align}
	\langle \rho\rangle &= \int_0^1\rho ~p(\rho) d\rho\\
	&= (N+1){N\choose A}\int_0^1 ~\rho^{A+1}(1-\rho)^{N-A} d\rho
\end{align}
Using the same beta function relation (or alternatively using the chain rule) we get
\begin{align}
	\langle \rho\rangle &= \int_0^1\rho ~p(\rho) d\rho\\
	&= (N+1)\frac{N!}{A!(N-A)!}\frac{(A+1)!(N-A)!}{(N+2)!}
\end{align}
Therefore, the chance that we get a heads on the next flip, is given by
\begin{align}
	\langle\rho\rangle = \frac{A+1}{N+2}
\end{align}

%\subsection{Uncertainties?}
%Study email from Jay "CLCT output to Track Finder" to understand why it makes sense to add errors in quadrature, and write it up here.


\subsection{Bayes Theorem}
The conditional probability, written as $P(A|B)$ is understood as the chance that $A$ happens, given that $B$ has already happened. The chance that both $A$ and $B$ happen $P(A \cap B)$ is the same as the chance $B$ happens at all $P(B)$, multiplied by $P(A|B)$.
\begin{align}
P(A\cap B) = P(A|B)P(B)
\end{align}
This can of course be looked at the other way, with
\begin{align}
P(A\cap B) = P(B|A)P(A)
\end{align}
Therefore
\begin{align}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{align}

\section{Likelihood Function}

The likelihood function is an interpretation of the probability distribution which takes the observed values $\textbf{x}$ as \textit{fixed} and allows the parameters $\boldsymbol{\mu}$ to vary.

\begin{align}
	P(\textbf{x}|\boldsymbol{\mu}) = \mathcal{L}(\boldsymbol{\mu}|\textbf{x})
\end{align}

\subsection{Maximum Likelihood Estimation}
The most foolproof means of solving an optimization problem is writing down the corresponding likelihood function, then solving for the parameters $\boldsymbol{\mu}$. If doing so analytically, this corresponds to solving the series of equations given by
\begin{align}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}} = 0
\end{align}
In general this usually isn't possible, so one resorts to other means such as gradient descent. When doing this optimization on a computer, typically one optimizes for
\begin{align}
	\ln\mathcal{L}
\end{align}
instead of $\mathcal{L}$. This is kosher since $\ln$ is a strictly increasing function, so maximizing it means maximizing its arguments \cite{burkov}. There are also some nice properties since 
\begin{align}
-2\ln\mathcal{L} \simeq \chi^2
\end{align}

% TODO Maximum Likelihood Estimation

This tells us that if the model had the parameter $\mu_{ML}$ the observation we made would be the most likely, out of the space of all possible $\mu$'s it could have been. To find the \textbf{confidence interval}, we use

\begin{align}
-2\ln\mathcal{L} \simeq \chi^2
\end{align}
This is because (TODO: Learn more about this) most things tend towards Gaussians in the limit of large numbers and in the product of many Likelihood functions one gets
\begin{align}
	-2\ln\prod_i\mathcal{L}_i = -2\ln\prod_i e^{(x_i-\mu)^2/2\sigma^2} = \sum_i\frac{(x_i-\mu)^2}{\sigma^2} = \chi^2
\end{align}
(TODO, $i$'s are correct?). Now using Wilk's Theorem (TODO), one can look at the change in $\chi^2$ as you change the value of $\mu$. Once you increase $\mu$ in one direction from $\mu_{ML}$, the $\chi^2$ will grow. $\chi^2$ will of course also grow if you decrease it from $\mu_{ML}$. You can quantify how much you can change $\mu$ to some limit of $\chi^2$ given by (SOME THEORY). Depending on what kind of confidence interval you are looking for, or the amount of parameters you still have, this change in $\chi^2$ will be different. 

This defines your confidence interval, which tells you, given whatever data you have, what the values of $\mu$,  [$\mu_1, \mu_2$] would be such that what you saw was on the tail end of its distribution (e.g. outside of 95\% of the entire distribution)

\subsection{Asimov Dataset}
An Asimov dataset is an artificial dataset such that when it is used to evaluate the estimators for all parameters in the context of maximum likelihood estimation, one obtains the (assumed) true parameter values. This is typically done to derive $\sigma$ of the parameter of interest $\mu$, which is then used in asymptotic formulae. Some notes on notation
\begin{itemize}
	\item $\hat{\mu}$: Best estimate on the parameter of interest $\mu$
	\item $\mu'$: Assumed true mean of the Gaussian used in Asymptotic formula approximations
\end{itemize}
This method is used to derive the approximate Gaussian width when calculating Asymptotic formulas for limits in particle physics \cite{cowan}. It was named after author, Isaac Asimov, whose 1955 short story, Franchise, envisaged the 2008 US Presidential Election decided by one voter representative of the entire electorate.



\section{Hypothesis Testing}

\subsection{Confidence Intervals}
A confidence interval provides a range of values that is likely to contain an unknown population parameter. For a confidence level of $(1-\alpha)$, we construct an interval $[\theta_L, \theta_U]$ such that:
\begin{align}
	P(\theta_L \leq \theta \leq \theta_U) = 1 - \alpha
\end{align}

For a sample mean $\bar{x}$ from a normal distribution with known variance $\sigma^2$, the $(1-\alpha)$ confidence interval is:
\begin{align}
	\bar{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}
\end{align}
where $z_{\alpha/2}$ is the critical value from the standard normal distribution. For a 95\% confidence interval, $z_{0.025} \approx 1.96$.

When the population variance is unknown and must be estimated from the sample, we use the $t$-distribution:
\begin{align}
	\bar{x} \pm t_{\alpha/2, n-1} \frac{s}{\sqrt{n}}
\end{align}
where $s$ is the sample standard deviation and $t_{\alpha/2, n-1}$ is the critical value from the $t$-distribution with $n-1$ degrees of freedom.

\subsubsection{Confidence Intervals for Fit Parameters}
When fitting a model to data using least squares or maximum likelihood, the confidence intervals for parameters can be estimated from the covariance matrix of the fit. For a parameter $\theta_i$, the standard error is:
\begin{align}
	\sigma_{\theta_i} = \sqrt{(\B{C})_{ii}}
\end{align}
where $\B{C}$ is the covariance matrix, often computed as the inverse of the Hessian of the negative log-likelihood evaluated at the best-fit point:
\begin{align}
	\B{C} = \left( -\frac{\partial^2 \ln \mathcal{L}}{\partial \theta_i \partial \theta_j} \right)^{-1}\Bigg|_{\hat{\theta}}
\end{align}

Alternatively, using the $\Delta\chi^2$ method, confidence regions can be found by identifying where:
\begin{align}
	\chi^2(\theta) - \chi^2_{\min} = \Delta\chi^2
\end{align}
For one parameter, $\Delta\chi^2 = 1$ gives the 68\% (1$\sigma$) interval, and $\Delta\chi^2 = 3.84$ gives the 95\% interval.


\url{https://online.stat.psu.edu/stat415/lesson/7/7.5}
\subsection{P-Value}
The $p$-value is defined as the probability under the null hypothesis of obtaining a result equal to or more extreme than what was actually observed. The smaller the $p$-value, the more it tells investigators that the null hypothesis may not adequately explain the observation.


\subsection{A/B Testing}
See book \cite{grus}


\subsection{F-Test}
The $F$-test is used to compare statistical models that have been fitted to a dataset, to determine whether a more complex model provides a significantly better fit than a simpler one. It is based on the $F$-distribution, which arises as the ratio of two scaled chi-squared distributions.

Given two models with residual sum of squares $\text{RSS}_1$ and $\text{RSS}_2$ (where model 2 is the more complex model with more parameters), the $F$-statistic is:
\begin{align}
	F = \frac{(\text{RSS}_1 - \text{RSS}_2) / (p_2 - p_1)}{\text{RSS}_2 / (n - p_2)}
\end{align}
where $p_1$ and $p_2$ are the number of parameters in each model, and $n$ is the number of data points. Under the null hypothesis (that the simpler model is adequate), $F$ follows an $F$-distribution with $(p_2 - p_1, n - p_2)$ degrees of freedom.

The $F$-test is also commonly used in ANOVA (Analysis of Variance) to test whether group means are equal:
\begin{align}
	F = \frac{\text{Between-group variability}}{\text{Within-group variability}} = \frac{\text{MSB}}{\text{MSW}}
\end{align}
where MSB is the mean square between groups and MSW is the mean square within groups.

A large $F$-value indicates that the between-group variability is large relative to the within-group variability, suggesting the group means are significantly different.

\section{Kalman Filter}
The Kalman filter\cite{fruwirth} uses a series of measurements taken in a time series to make an estimate of the underlying parameters. It works as an iterative $\chi^2$ method, and is useful when there is noise introduced between measurements.

Let us assume the state vector of interest is $\textbf{x}$. If we assume the evolution of the state vector is linear, we have
\begin{align}
    \textbf{x}_k = \textbf{F}_{k-1}\textbf{x}_{k-1} + w_{k-1}
\end{align}
Where $k$ is the current state, $w$ is random noise that is introduced, and $\textbf{F}$ is how the state vector propagates itself (here it is assumed to be linear). Measurements $\textbf{m}$ in each detector $k$ can be written as
\begin{align}
    \textbf{m}_k = \textbf{H}_k\textbf{x}_k + \epsilon_k
\end{align}
We assume both $w_k$ and $\epsilon_k$ are independent and have a mean of zero. The Kalman algorithm performs three different analyses:
\begin{enumerate}
    \item \textbf{Filtering}: estimation of present state vector based off of all previous measurements.
    \item \textbf{Prediction}: estimation of future state vector.
    \item \textbf{Smoothing}: estimation of the state vector at some time in the past using all information known in the present.
\end{enumerate}
The Kalman filter minimizes the mean squared error and is in that sense the optimum solution to these problems. We can denote our estimate $\hat{\textbf{x}}_k^i$ for each of these three cases:
\begin{itemize}
    \item $i < k$: prediction, since we are using fewer points than the state we are looking at $k$
    \item $i=k$: filtering, since we are using all previous states and the measurement at $k$ itself
    \item $i > k$: smoothing, since we already had a measurement at this point, but used additional later values to get a better estimate
\end{itemize}
One can then define the covariance matrix as the difference between the ...



\section{Bootstrapping}
Bootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly drawing samples \emph{with replacement} from the observed data. This allows estimation of standard errors, confidence intervals, and other properties without making strong parametric assumptions about the underlying distribution.

\subsection{The Bootstrap Algorithm}
Given a sample of size $n$, the bootstrap procedure is:
\begin{enumerate}
	\item Draw $n$ observations \emph{with replacement} from the original sample to create a bootstrap sample
	\item Compute the statistic of interest $\hat{\theta}^*$ on the bootstrap sample
	\item Repeat steps 1-2 a large number of times $B$ (typically $B \geq 1000$)
	\item Use the distribution of $\hat{\theta}^*_1, \hat{\theta}^*_2, \ldots, \hat{\theta}^*_B$ to estimate properties of $\hat{\theta}$
\end{enumerate}

\subsection{Bootstrap Standard Error}
The bootstrap estimate of the standard error is simply the standard deviation of the bootstrap distribution:
\begin{align}
	\widehat{\text{SE}}_\text{boot} = \sqrt{\frac{1}{B-1}\sum_{b=1}^{B}(\hat{\theta}^*_b - \bar{\theta}^*)^2}
\end{align}
where $\bar{\theta}^* = \frac{1}{B}\sum_{b=1}^B \hat{\theta}^*_b$.

\subsection{Bootstrap Confidence Intervals}
Several methods exist for constructing bootstrap confidence intervals:

\paragraph{Percentile Method} The $(1-\alpha)$ confidence interval is taken directly from the percentiles of the bootstrap distribution:
\begin{align}
	[\hat{\theta}^*_{(\alpha/2)}, \hat{\theta}^*_{(1-\alpha/2)}]
\end{align}

\paragraph{Basic Bootstrap} Also called the ``pivotal'' method:
\begin{align}
	[2\hat{\theta} - \hat{\theta}^*_{(1-\alpha/2)}, 2\hat{\theta} - \hat{\theta}^*_{(\alpha/2)}]
\end{align}

\paragraph{BCa (Bias-Corrected and Accelerated)} This method adjusts for both bias and skewness in the bootstrap distribution and generally provides better coverage than the percentile method.

\subsection{When Bootstrapping Works}
Bootstrapping is particularly useful when:
\begin{itemize}
	\item The theoretical distribution of the statistic is unknown or complex
	\item Sample sizes are small and parametric assumptions may not hold
	\item You need confidence intervals for complicated statistics (medians, ratios, etc.)
\end{itemize}

However, bootstrapping can fail when the statistic is not ``smooth'' (e.g., sample maximum) or when the sample size is too small to adequately represent the population.

