\chapter{Computation}

\subsection{Complexity}

Big-O notation is typically used to approximate the time dependence on the number of elements you feed into an algorithm. In Big-O notation, we ignore all lower order terms, and also forget about any leading constants.

Listed are some standard algorithms and their dependence.

\begin{center}
\begin{tabular}{ | c | c|} 
\hline
 $N$-Dependence & Algorithm\\ \hline
$O(1)$ & Accessing an element in an array  \\ 
$O(\log N)$ & Binary Search  \\
$O(N)$ & Single \texttt{for} loop \\ 
$O(N^2)$ & Simple sorting algorithm (bubble, selection, ...)\\
$O(c^N)$ & Solving the traveling salesman problem with dynamic programming \\
$O(N!)$ & Iterations over all combinatorics \\
\hline
\end{tabular}
\end{center}

\subsection{Binary Representations}

Computers typically represent information in binary. Knowing roughly how large objects are in memory is useful when solving problems. A sequence of bits represents powers of 2, which can be arranged to represent all integer values from 0 to $2^N-1$ where $N$ is the amount of bits you have. 
\begin{center}
\begin{tabular}{ | c | c| c|} 
\hline
 bits & unique values & label \\ \hline
7 & $128$ &  \\ 
8 & $256$ &  \\ 
10 & $1,024$ & KB \\ 
20 & $1,048,576$ & MB \\ 
30 & $1,073,741,824$ & GB \\ 
\hline
\end{tabular}
\end{center}

Adding $10$ to a power of two increases the amount of memory used by roughly a factor of $1000$.

There are slight subtleties involving whether we are counting using binary prefixes (e.g. kibibyte = KB) or decimal prefixes (e.g. kilobyte = kB). Binary prefixes count in units of 1024 (a power of two, near 1000), whereas decimal prefixes count in powers of 10. 

\subsection{Efficient Algorithms}

\subsection{Averaging}
When updating an average which contains many elements, the naive way of calculating the average is $O(n)$ efficient 
\begin{align}
	\langle x_{n-1}\rangle = \frac{1}{n-1}\sum_{i=0}^{n-1} x_i
\end{align}
When constantly updating the average, we can use the following equivalent formula
\begin{align}
	\langle x_{n}\rangle &= \frac{1}{n}\sum_{i=0}^{n} x_i\\
	&= \frac{1}{n}\Big(x_n + (n-1)\langle x_{n-1}\rangle\Big)\\
	&= \langle x_{n-1}\rangle + \frac{1}{n}\Big(x_n - \langle x_{n-1} \rangle\Big)
\end{align}
This modified algorithm is $O(1)$ efficient.

\subsection{Estimating Probabilities}
When multiplying many different probabilities $p_i$ together, the floating point precision of many computing languages starts to become an issue. To avoid this problem, often $\log(p_i)$ is used which is monotonic with the probability but avoids the \textit{underflow} issue described.

\subsection{Graph Theory}
Brute force algorithms that search all combinations of $N$ items scale as $N!$. The Seven Bridges of Konigsberg is a famous example solved by Euler, for which he laid our all combinations of sequences you could possibly cross the seven bridges (e.g. $1234567, 1234576,...$ and was able to show each combination did not work.


\subsection{Kernels}
Kernels are useful when one wants to represent a set of data points in a higher dimensional space (e.g. when making a classifier using Support Vector Machines). One represents the transformation of a set of data points $\B{x}$ as $\B{x}\rightarrow \phi(\B{x})$. Mathematically a Kernel function is defined as
\begin{align}
	K(\B{x},\B{x}') = \phi(\B{x})\cdot \phi(\B{x}')
\end{align}

 Kernels themselves are a measure of similarity between two different points of data $\B{x}$ and $\B{x}'$ (e.g. cosine similarity). Their main utility lies in the fact that cleverly defined kernels can significantly reduce the amount of computation necessary when one wants to expand the dimensionality of the dataset. It happens that many algorithms in machine learning can be written in terms of dot products, which allows kernel methods to be used if one wants change dimensionality.

\subsubsection{Polynomial Kernel}
The polynomial kernel is defined as
\begin{align}\label{pol_kern}
	K_d(\B{x},\B{x}') = (1+ \B{x}\cdot \B{x}')^d
\end{align}
Where $d$ is the dimension you wish to expand the data to. To illustrate the utility of the kernel, let us take
\begin{align}\label{pol_kern_example}
	\B{x} = (x_1,x_2) && \B{x}' = (x_1',x_2')
\end{align}
If we want to increase the dimensionality of our data to consider all second order terms and below, one can define the transformation function as
\begin{align}
	\phi(\B{x}) = (1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2)
\end{align}
If it happens that each time the data shows up, a dot product is used, then the condition for using a kernel condition is satisfied.
\begin{align}
	\B{x}\cdot \B{x}' \rightarrow \phi(\B{x})\cdot \phi(\B{x}')
\end{align}
Let us then compute this dot product for Equation \ref{pol_kern_example}
\begin{align}
	\phi(\B{x})\cdot \phi(\B{x}') &= (1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2) \\
	&\cdot (1, \sqrt{2}x_1', \sqrt{2}x_2', x_1^{'2}, x_2^{'2}, \sqrt{2}x_1'x_2')\\
	&= 1 + 2x_1x_1' + 2x_2x_2' + x_1^2x_1^{'2}+x_2^2x_2^{'2} + 2x_1x_2x_1'x_2' \label{pol_kern_expand}
\end{align}
Now let's think about what the computer actually has to do to compute all of these terms. We would first have to calculate each term for $\phi(\B{x})$ and $\phi(\B{x}')$, which requires \emph{twelve} multiplications. Next we would have to compute the dot product of $\phi(\B{x}')\cdot\phi(\B{x}')$, which requires another \emph{six} multiplications followed by \emph{five} additions. In total, for just increasing the dimensionality of the data by one degree, we have 18 multiplication and 5 addition operations involved for each set of points.

However, one can identify that using the polynomial kernel definition of Equation \ref{pol_kern} for $d=2$ gives the same result
\begin{align}
	(1+ \B{x}\cdot \B{x}')^2 &= \Big(1+(x_1,x_2)\cdot(x_1',x_2')\Big)^2\\
	&= \Big(1+x_1x_1'+x_2x_2'\Big)^2\\
	&= 1 + 2x_1x_1' + 2x_2x_2' + x_1^2x_1^{'2}+x_2^2x_2^{'2} + 2x_1x_2x_1'x_2'
\end{align}
This final expression is \emph{exactly equivalent} to that of Equation \ref{pol_kern_expand}, however, let us think about how many operations were necessary to actually compute the numerical value. We begin with \emph{two} multiplications followed by \emph{two} additions, followed by \emph{one} multiplication. In total, by expression function this way, we get away with only \emph{three} multiplication and \emph{two} addition operations. This reduction in overhead can pay off big time, especially when expanding to even higher dimensions.

\subsubsection{Radial Basis Kernel}
The radial basis kernel is defined as 
\begin{align}
		K(\B{x},\B{x}') = \exp\Big(-\frac{||\B{x}-\B{x}'||^2}{2\sigma^2}\Big)
\end{align}
This metric is a measure of similarity, when the two vectors are identical, the kernel evaluates to 1, while if they are infinitely far apart, it evaluates to 0.



\subsection{Quantum Computation}
\subsubsection{Grovers Algorithm}
%\url{https://en.wikipedia.org/wiki/Grover%27s_algorithm}